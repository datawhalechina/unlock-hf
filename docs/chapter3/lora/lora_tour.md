# LoRA: 大语言模型的低秩适配

## 前言

在自然语言处理(NLP)领域,一个重要的范式是在通用领域数据上进行大规模预训练,然后适配到特定任务或领域。随着我们预训练越来越大的模型,全参数微调(即重新训练所有模型参数)变得越来越不可行。以GPT-3 175B为例 - 为每个下游任务部署独立的微调模型实例,每个实例都有175B参数,这在计算和存储上都是极其昂贵的。

为了解决这个问题,微软研究院提出了[低秩适配(Low-Rank Adaptation, LoRA)](https://arxiv.org/pdf/2106.09685v2)方法。LoRA冻结预训练模型的权重,并将可训练的低秩分解矩阵注入到Transformer架构的每一层中,大大减少了下游任务的可训练参数数量。

## LoRAl:小模型有大智慧(2021)

为了使微调更加高效，LoRA的方法是通过低秩分解将权重更新表示为两个较小的矩阵(称为更新矩阵，即图中的A和B)。这些新矩阵可以在适应新数据的同时保持整体变化数量较少进行训练。

原始权重矩阵保持冻结状态，并且不再接受任何进一步的调整。最终结果是通过将原始权重和适应后的权重进行组合得到,下图展示了LoRA的核心结构：
1. **输入层 (x)**: 
   - 底部黄色长条，维度为 d

2. **预训练权重 (W)**: 
   - 左侧蓝色方块
   - W ∈ ℝ ^ (d×d)
   - 在微调过程中保持冻结

3. **LoRA更新矩阵**:
   - 右侧橙色部分
   - 包含两个低秩矩阵 A 和 B
   - A 初始化为 N(0, σ^2)
   - B 初始化为 0
   - r 表示矩阵的秩，通常 r << d

4. **组合过程**:
   - 顶部的 "+" 符号
   - 将预训练权重与LoRA更新矩阵的乘积相加

5. **输出 (h)**:
   - 顶部黄色长条
   
<div style="text-align: center;">
    <img src="imgs/lora.png" alt="LoRA" width="300px">
</div>

## LoRA核心技术解密
在LORA方法中，实际上是在原始预训练语言模型(PLM)旁增加一个附加的网络通路这可以视作一种“外挂”结构。这个外挂结构的目的是通过两个短阵A和B的相乘来模拟本征秩(intrinsic rank)。  

$$h = W_0x + \Delta Wx = W_0x + BAx$$

### 3.1 数学表示

对于预训练权重矩阵 $W_0 \in \mathbb{R}^{d \times k}$,LoRA通过以下方式对其进行更新:

$W = W_0 + BA$

其中:
- $B \in \mathbb{R}^{d \times r}$
- $A \in \mathbb{R}^{r \times k}$
- $r$ 是秩,且 $r \ll \min(d,k)$

在训练过程中:
- $W_0$ 被冻结,不接受梯度更新
- A 和 B 包含可训练参数

对于输入 $x$, 前向传播过程如下:

$h = W_0x + BAx$

### 3.2 初始化和缩放

- A 使用高斯随机初始化
- B 初始化为零矩阵
- $BAx$ 被缩放因子 $\alpha/r$ 缩放,其中 $\alpha$ 是一个常数

### 3.3 应用到Transformer

LoRA主要应用于Transformer架构中的自注意力模块。具体来说:

- 在每个Transformer层中,LoRA被应用到查询(Q)、键(K)、值(V)和输出(O)投影矩阵
- MLP模块通常被冻结,不进行适配

## 4. LoRA的优势

1. **参数效率高**: LoRA可以将可训练参数数量减少10,000倍。以GPT-3 175B为例,LoRA将可训练参数从350GB减少到了35MB。

2. **内存效率高**: 由于大部分参数被冻结,LoRA可以将GPU内存需求减少3倍。对于GPT-3 175B,内存消耗从1.2TB降低到了350GB。

3. **推理延迟低**: 与Adapter等方法不同,LoRA不引入额外的推理延迟。在部署时,可以将训练好的A和B矩阵与原始权重合并。

4. **任务切换成本低**: 可以通过只交换LoRA权重(而不是所有参数)来实现低成本的任务切换。

5. **训练速度快**: 由于不需要计算大部分参数的梯度,LoRA在GPT-3 175B上比全参数微调快25%。

6. **性能comparable**: 尽管参数量大幅减少,LoRA在多个下游任务上的表现与全参数微调相当或更好。

## 5. LoRA的实现细节

### 5.1 rank的选择

实验表明,即使是非常小的rank(如r=1或r=2)也能在很多任务上取得不错的效果。这进一步证实了更新矩阵确实具有很低的"内在秩"。

### 5.2 应用LoRA的权重选择

给定有限的参数预算,应该将LoRA应用到哪些权重矩阵以获得最佳性能?实验结果表明:

- 同时适配查询(Q)和值(V)矩阵通常能获得最佳效果
- 仅适配查询(Q)或键(K)矩阵效果较差
- 适配所有注意力权重(Q,K,V,O)在某些任务上可能会带来进一步的性能提升

### 5.3 缩放因子α的作用

缩放因子α的引入有助于:

1. 使不同rank r的模型初始化保持一致性
2. 允许使用与全参数微调相近的学习率,简化超参数调优过程

## 6. LoRA的理论基础

LoRA的有效性可以从以下几个角度来理解:

1. **过参数化与低秩结构**: 深度学习模型通常是高度过参数化的,而过参数化模型往往具有低秩性质。LoRA正是利用了这一特性。

2. **任务特定特征放大**: LoRA本质上是在放大预训练模型中已经学到但未被充分强调的、对特定任务重要的特征方向。

3. **优化landscape的低维结构**: 研究表明,即使在高维参数空间中,模型优化实际上也是在一个低维子空间中进行的。LoRA正是捕捉了这个低维子空间。

## 7. LoRA与其他PEFT方法的比较

相比其他参数高效微调方法,LoRA具有以下优势:

1. **vs Adapter**: 不引入额外推理延迟,参数效率更高。
2. **vs Prefix-tuning**: 不占用输入序列长度,优化更稳定。
3. **vs BitFit**: 可调参数更多,表达能力更强。

同时,LoRA也可以与其他PEFT方法(如Prefix-tuning)结合使用,potentially带来互补的性能提升。

## 8. 实践建议

1. **选择合适的rank**: 从小的rank(如1或2)开始尝试,根据任务需求逐步增加。
2. **权重选择**: 优先考虑同时适配查询(Q)和值(V)矩阵。
3. **与其他技术结合**: 可以尝试将LoRA与其他PEFT方法结合使用。
4. **缩放因子调优**: 可以尝试不同的α值,但通常默认值就能工作得很好。

## 9. 结论

LoRA作为一种简单而有效的参数高效微调方法,为大规模语言模型的高效适配提供了一种重要的解决方案。它不仅大大降低了计算和存储成本,还保持了与全参数微调相当的性能。随着模型规模的不断增长,LoRA及其衍生方法必将在未来的NLP应用中发挥越来越重要的作用。

## 参考文献

1. Edward J. Hu, et al. "LoRA: Low-Rank Adaptation of Large Language Models." arXiv preprint arXiv:2106.09685 (2021).

2. Neil Houlsby, et al. "Parameter-Efficient Transfer Learning for NLP." ICML 2019.

3. Xiang Lisa Li and Percy Liang. "Prefix-Tuning: Optimizing Continuous Prompts for Generation." ACL 2021.

4. Elad Ben-Zaken, et al. "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models." ACL 2022.