---
comments: true
title: PEFTæ€»è§ˆ
---


# PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰

## ä»€ä¹ˆæ˜¯ PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰

ğŸ¤— `PEFT`ï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆåœ°å°†å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹é€‚é…åˆ°å„ç§ä¸‹æ¸¸åº”ç”¨çš„åº“ã€‚

ç”±äºå¾®è°ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥å®ç°ï¼Œè€Œ `PEFT` æ–¹æ³•åªéœ€è¦å¾®è°ƒå°‘é‡é¢å¤–æ¨¡å‹å‚æ•°ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ï¼ŒåŒæ—¶å®ç°äº†ä¸æ¨¡å‹å…¨é‡å¾®è°ƒ**è¿‘ä¹ç›¸å½“**çš„æ€§èƒ½ã€‚è¿™ä½¿å¾—åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè®­ç»ƒå’Œä¿å­˜å¤§è¯­è¨€æ¨¡å‹ (LLM) å˜å¾—æ›´åŠ å®¹æ˜“ã€‚

å®‰è£… `peft`

```bash title='å®‰è£…peft'
pip install peft
```

PEFT æ¶µç›–ä¼—å¤šä¸»æµä½å‚é«˜æ ¡å¾®è°ƒæŠ€æœ¯ï¼Œå¹¶å¯ä»¥å’Œ `Transformers`ã€`Accelerate` ä¸€èµ·ä½¿ç”¨ï¼Œæ¯”å¦‚

1. `LoRa`
2. `Prefix Tuning`
3. `AdaLoRA`
4. `Prompt Tuning`
5. `MultiTask Prompt Tuning`
6. `LoHa`
7. $\cdots$

`PEFT` åº“æ”¯æŒçš„æ–¹æ³•å‡å¯åœ¨ [Adapters HuggingFace](https://huggingface.co/docs/peft/main/en/package_reference/adalora#peft.AdaLoraConfig) å·¦ä¾§å¯¼èˆªæ æŸ¥æ‰¾

æ¥ä¸‹æ¥çš„å†…å®¹å°†ä»‹ç» `PEFT` çš„ä¸»è¦ç»„æˆï¼Œä»¥åŠå¦‚ä½•è®­ç»ƒæˆ–è¿è¡Œé‚£äº›é€šå¸¸åœ¨æ¶ˆè´¹çº§è®¾å¤‡ä¸Šéš¾ä»¥è®­ç»ƒçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚

## `PeftConfig`

æ¯ç§ `PEFT` æ–¹æ³•éƒ½å¯¹åº”ä¸€ä¸ªç‹¬ç‰¹çš„ `PeftConfig` ç±»ï¼Œç”¨äºå­˜å‚¨æ„å»º**ç›¸åº”** `PeftModel` çš„æ‰€æœ‰å¿…è¦å‚æ•°ã€‚

å½“ä½ æƒ³è¦è°ƒç”¨æŸä¸ª `PEFT` æ–¹æ³•æ—¶ï¼Œéœ€è¦å…ˆåŠ è½½å¹¶åˆ›å»ºä¸€ä¸ªè¯¥æ–¹æ³•å¯¹åº”çš„ `PeftConfig` ç±»å®ä¾‹ï¼Œå¹¶åœ¨å®ä¾‹åŒ–è¿‡ç¨‹ä¸­æŒ‡å®šè¯¥æ–¹æ³•éœ€è¦çš„å‚æ•°ã€‚è¿™äº›å‚æ•°ä¼šå›  `PEFT` æ–¹æ³•çš„ä¸åŒè€Œæœ‰æ‰€å·®å¼‚ï¼Œä¾‹å¦‚ï¼š

- `LoRa` (`LoraConfig`)ï¼šéœ€è¦æŒ‡å®šÂ `lora_rank`ï¼ˆä½ç§©çŸ©é˜µçš„ç§©ï¼‰ã€`lora_alpha`ï¼ˆç¼©æ”¾å› å­ï¼‰å’ŒÂ `lora_dropout`ï¼ˆdropout æ¦‚ç‡ï¼‰ç­‰å‚æ•°ã€‚
- `Prompt Tuning` (`PromptTuningConfig`)ï¼šéœ€è¦æŒ‡å®šÂ `prompt_tuning_num_tokens`ï¼ˆprompt ä¸­çš„ token æ•°é‡ï¼‰ã€`prompt_tuning_init_text`ï¼ˆprompt çš„åˆå§‹åŒ–æ–‡æœ¬ï¼‰å’ŒÂ `prompt_tuning_placeholder_id`ï¼ˆå ä½ç¬¦ IDï¼‰ç­‰å‚æ•°ã€‚

å‡å¦‚ä»¥ `LoRa` ä¸ºä¾‹å­ï¼Œæ€ä¹ˆè®© `LoRa` ä½œç”¨äºæ¨¡å‹å‘¢ï¼Ÿ

1. å¼•å…¥Â `LoraConfig`Â ç±»ã€‚
2. å®šä¹‰ `LoRa` çš„å‚æ•°ï¼ŒåŒ…æ‹¬Â `task_type`ï¼Œ`inference_mode`ï¼Œ`r`ï¼Œ`lora_alpha`Â å’ŒÂ `lora_dropout` ç­‰ã€‚

```python title='LoraConfig'
from peft import LoraConfig, TaskType

peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)
```

è¿™é‡Œç®€å•ä»‹ç»è¿™äº›å‚æ•°ï¼š

- `Task_type`ï¼šä¸‹æ¸¸ä»»åŠ¡ç±»å‹ï¼Œé¢å¯¹ä¸åŒçš„ä»»åŠ¡ç±»å‹ï¼ŒPEFT æ–¹æ³•å°†ä»¥ä¸åŒçš„æ–¹å¼è°ƒæ•´æ¨¡å‹ã€‚
    - `SEQ_CLS` å¯¹åº”æ–‡æœ¬åˆ†ç±»
    - `SEQ_2_SEQ_LM` å¯¹åº”åºåˆ—åˆ°åºåˆ—çš„è¯­è¨€æ¨¡å‹
    - `CAUSAL_LM` å¯¹åº”å› æœå…³ç³»æ¨¡å‹
    - `TOKEN_CLS` å¯¹åº” Token åˆ†ç±»
    - `QUESTION_ANS` å¯¹åº”é—®ç­”é—®é¢˜
    - `FEATURE_EXTRACTION` å¯¹åº”ç‰¹å¾æå–
    - $\cdots$
    - æ‰€æœ‰çš„ä»»åŠ¡ç±»å‹éƒ½å¯ä»¥åœ¨å®˜æ–¹æ–‡æ¡£æŸ¥è¯¢ [Task_type Huggingface](https://huggingface.co/docs/peft/main/en/package_reference/peft_types#peft.TaskType)

- `Inference_mode`ï¼šæ˜¯ä¸€ç§ç”¨äºä¼˜åŒ–æ¨¡å‹æ¨ç†é˜¶æ®µæ€§èƒ½çš„æœºåˆ¶ã€‚
    - å½“æ¨¡å‹éœ€è¦è®­ç»ƒæ—¶ï¼Œå°†å…¶è®¾ç½®ä¸º `False`ï¼Œä»¥æ­£å¸¸è¿›è¡Œæ¢¯åº¦æ›´æ–°ã€å¯ç”¨è®­ç»ƒé˜¶æ®µç‰¹æœ‰çš„æ“ä½œï¼ˆ`Dropout` å’Œ `Batch Normalization` ç­‰ï¼‰ã€‚
    - å½“æ¨¡å‹éœ€è¦æ¨ç†æ—¶ï¼Œå°†å…¶è®¾ç½®ä¸º `True`ï¼Œä»¥ç¦ç”¨æ¢¯åº¦æ›´æ–°ï¼Œé‡Šæ”¾å†…å­˜ç©ºé—´ï¼Œæé«˜æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¹Ÿä¼šè·³è¿‡è®­ç»ƒé˜¶æ®µç‰¹æœ‰çš„æ“ä½œï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§ã€‚

- `r`ï¼šä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œå…¶å€¼è¶Šå°ï¼Œ`LoRa` æ·»åŠ çš„å‚æ•°å°±è¶Šå°‘ï¼Œæ¨¡å‹è®­ç»ƒé€Ÿåº¦å°±è¶Šå¿«ï¼Œä½†ä¹Ÿå¯èƒ½é™ä½æ¨¡å‹çš„æ€§èƒ½ã€‚
- `lora_alpha`ï¼šä½ç§©çŸ©é˜µçš„ç¼©æ”¾å› å­ã€‚å…¶å€¼è¶Šå¤§ï¼Œ`LoRa` å¯¹æ¨¡å‹çš„å½±å“å°±è¶Šå¤§ã€‚
- `lora_dropout`ï¼šåº”ç”¨äº `LoRa` å±‚çš„ `dropout` æ¦‚ç‡ï¼Œå…¶åœ¨é€‚å½“çš„æ•°å€¼èŒƒå›´å†…å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
- `target_modules`ï¼šé€‰æ‹©è¦åº”ç”¨é€‚é…å™¨çš„æ¨¡å—ï¼Œå¯ä»¥é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ã€ç²¾ç¡®åŒ¹é…ã€æ¨¡å—åç§°ç»“å°¾åŒ¹é…æˆ–é€‰æ‹©æ‰€æœ‰çº¿æ€§å±‚æ¥å®ç°ã€‚å¦‚æœæœªæŒ‡å®šè¯¥å‚æ•°ï¼ŒPEFT ä¼šæ ¹æ®æ¨¡å‹æ¶æ„è‡ªåŠ¨é€‰æ‹©ç›®æ ‡æ¨¡å—ã€‚å¦‚æœæ— æ³•è¯†åˆ«æ¨¡å‹æ¶æ„ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šç›®æ ‡æ¨¡å—ã€‚æ‰€æœ‰é»˜è®¤çš„å¾®è°ƒæ¨¡å—éƒ½å¯ä»¥åœ¨ [peft.utils.constants](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py) æŸ¥çœ‹ã€‚

ä¸Šé¢çš„ä¾‹å­æ˜¯é’ˆå¯¹äº `LoRa` çš„ï¼Œä½†æ˜¯ç°å®ä¸­å¯èƒ½éœ€è¦æ›´å¤šä¸åŒçš„ `PEFT` æ–¹æ³•, ä¸åŒçš„ `PEFT` æ–¹æ³•åˆéœ€è¦æŒ‡å®šä¸åŒçš„å‚æ•°ï¼Œåœ¨ä¸äº†è§£éœ€è¦ä»€ä¹ˆå‚æ•°çš„æ—¶å€™æ€ä¹ˆæ“ä½œå‘¢ï¼Ÿ

æ‰€æœ‰çš„å¾®è°ƒæ–¹æ³•çš„é…ç½®åŠå…¶å‚æ•°ä»‹ç»éƒ½èƒ½åœ¨ [Adapters HuggingFace](https://huggingface.co/docs/peft/main/en/package_reference/adalora#peft.AdaLoraConfig) å·¦ä¾§å¯¼èˆªæ è¢«æ‰¾åˆ°ã€‚

è¿›å…¥è¯¦ç»†ä»‹ç»ç•Œé¢ï¼Œæ•´ä½“èƒ½çœ‹åˆ°ç›¸åº” `PEFT` æ–¹æ³•çš„ä»‹ç»
![lora_config](./imgs/lora_config.png)

å…¶æ¬¡æ˜¯ç›¸åº” `PEFT` æ–¹æ³•çš„å†…ç½®çš„å‚æ•°ï¼Œåœ¨ `Parameters` æ åˆ—å‡ºäº†æœ€ä¸ºé‡è¦çš„å‚æ•°ï¼Œä½¿ç”¨è€…å¯ä»¥æ ¹æ®è¯´æ˜åŠéœ€æ±‚è‡ªå®šä¹‰ç›¸å…³å‚æ•°ã€‚

![lora_parameters](./imgs/lora_parameters.png)

---

## `PeftModel`

è®¾ç½®å®Œ `PeftConfig` åï¼Œç„¶åä½¿ç”¨ `get_peft_model()` å‡½æ•°åˆ›å»º `PeftModel`ã€‚ `get_peft_model()` éœ€è¦ä¸€ä¸ªä» `transformers` åŠ è½½çš„åŸºç¡€æ¨¡å‹å’Œå·²ç»å®šä¹‰å¥½çš„ `PeftModel` å®ä¾‹ã€‚

```python title='base model'
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/mt0-large")
```

ä½¿ç”¨Â `get_peft_model()`Â å’ŒÂ `peft_config`Â åˆ›å»ºÂ `PeftModel`Â æ˜¯ä½¿ç”¨ `PEFT` çš„æ ‡å‡†æ–¹æ³•ã€‚

```python title='LoraModel'
from peft import get_peft_model

model = get_peft_model(model, peft_config)
```

`PeftModel` æ‹¥æœ‰è®¸å¤šå†…ç½®çš„å±æ€§å’Œæ–¹æ³•ï¼Œè¿™é‡Œä¸»è¦ä»‹ç»ä»¥ä¸‹ä¸‰ç§ã€‚

1. `peft_model.base_model`: è®¿é—®åŸºç¡€æ¨¡å‹
2. `peft_model.print_trainable_parameters()`: æ‰“å°å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å’Œåç§°
3. `peft_model.save_pretrained()`: ä¿å­˜Â `PeftModel`ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’Œ `PEFT` é€‚é…å™¨

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å…¨é‡å¾®è°ƒä¸ä½¿ç”¨ä½å‚é«˜æ•ˆå¾®è°ƒæ—¶ï¼Œå‚ä¸æ¢¯åº¦æ›´æ–°çš„å‚æ•°å¯¹æ¯”å§ã€‚

```text title='model.print_trainable_parameters()'
"output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"
```

`bigscience/mt0-large` æ¨¡å‹æ‹¥æœ‰ $12$ äº¿å‚æ•°ï¼Œè€Œæˆ‘ä»¬åªéœ€è¦å¾®è°ƒå…¶ä¸­ $0.19%$ å°±èƒ½å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœï¼æ€»çš„æ¥è¯´é¢å¯¹åºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œ`PEFT` å·§å¦™åœ°å†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼Œåªå¾®è°ƒå°‘é‡çš„é¢å¤–å‚æ•°ï¼Œå°±èƒ½å–å¾—ä¸å…¨é‡å¾®è°ƒç›¸å½“ç”šè‡³æ›´å¥½çš„æ•ˆæœã€‚è¿™æ˜¯ä¸€é¡¹å¤šä¹ˆä»¤äººå¿ƒæƒ…æ„‰æ‚¦çš„äº‹æƒ…ï¼

## è®­ç»ƒ

ğŸ‰ åˆ°ç°åœ¨å·²ç»æˆåŠŸåœ°è®¾ç½®å¥½äº†è¢« `PEFT` æ–¹æ³•åŒ…è£¹åçš„æ¨¡å‹äº†ï¼Œå¹¶ä¸”å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒäº†ï¼æ¥ä¸‹æ¥å°±å¯ä»¥ä½¿ç”¨ `Trainer`, `Accelerate`, æˆ–è€…è‡ªå®šä¹‰çš„ `PyTorch` çš„è®­ç»ƒæµç¨‹ã€‚

è®­ç»ƒéƒ¨åˆ†ä¸æ˜¯æœ¬èŠ‚çš„é‡ç‚¹ï¼Œæ•…ç›´æ¥å¼•ç”¨å®˜æ–¹çš„ä»£ç ã€‚

```python title='training_args'
training_args = TrainingArguments(
    output_dir="your-name/bigscience/mt0-large-lora",
    learning_rate=1e-3,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)
```

ç°åœ¨å°±å¯ä»¥æŠŠæ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€æ•°æ®é›†ã€åˆ†è¯å™¨å’Œå…¶ä»–å¿…è¦ç»„ä»¶ç»Ÿç»Ÿæ‰”ç»™ `Trainer` ï¼Œç„¶åè°ƒç”¨ `train()` æ–¹æ³•å¼€å§‹è®­ç»ƒï¼

```python title='Trainer'
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

## ä¿å­˜æ¨¡å‹ã€æ¨ç†

### ä¿å­˜

å½“æ¨¡å‹å®Œæˆè®­ç»ƒåï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Â `model.save_pretrained()`Â å‡½æ•°å°†å…¶ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ä¸­ã€‚

### æ¨ç†

æ— è®ºæ˜¯è‡ªå·±è¿˜æ˜¯åˆ«äººä½¿ç”¨ `PEFT` è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ï¼Œåªè¦æ‹¿åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œå°±å¯ä»¥ä½¿ç”¨Â `AutoPeftModel`Â ç±»åŠå…¶Â `from_pretrained`Â æ–¹æ³•è½»æ¾åŠ è½½ `PEFT` è®­ç»ƒçš„æ¨¡å‹ä»¥è¿›è¡Œæ¨ç†ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ç§æ— ç¼çš„æ–¹å¼æ¥åŠ è½½å’Œä½¿ç”¨ä½ çš„å¾®è°ƒæ¨¡å‹ï¼Œè€Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹æ¶æ„æˆ– `PEFT` é…ç½®ã€‚

```python title='PeftModel infer'
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer
import torch

model = AutoPeftModelForCausalLM.from_pretrained("ybelkada/opt-350m-lora")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

model = model.to("cuda")
model.eval()
inputs = tokenizer("Preheat the oven to 350 degrees and place the cookie dough", return_tensors="pt")

outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=50)
print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])
```

```text title='output'
"Preheat the oven to 350 degrees and place the cookie dough in the center of the oven. In a large bowl, combine the flour, baking powder, baking soda, salt, and cinnamon. In a separate bowl, combine the egg yolks, sugar, and vanilla."
```

### æ³¨

- æˆ‘ä»¬æ—¢å¯ä»¥åœ¨è®­ç»ƒå®Œæˆåç«‹å³ä½¿ç”¨è®­ç»ƒå¥½çš„ `PEFT` æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä¹Ÿå¯ä»¥å°†æ¨¡å‹ä¿å­˜åˆ°ç£ç›˜ï¼Œç¨åå†åŠ è½½å®ƒè¿›è¡Œæ¨ç†ã€‚é€‰æ‹©å“ªç§æ–¹æ³•å–å†³äºä½ çš„å…·ä½“éœ€æ±‚ã€‚å¦‚æœåªæ˜¯æƒ³å¿«é€Ÿæµ‹è¯•æ¨¡å‹ï¼Œé‚£ä¹ˆç¬¬ä¸€ç§æ–¹æ³•æ›´æ–¹ä¾¿ã€‚å¦‚æœéœ€è¦é•¿æœŸä¿å­˜å’Œç®¡ç†æ¨¡å‹ï¼Œé‚£ä¹ˆç¬¬äºŒç§æ–¹æ³•æ›´åˆé€‚ã€‚

## å‚è€ƒèµ„æ–™

<div class="grid cards" markdown>

- `PEFT`æ”¯æŒçš„å¾®è°ƒæ–¹æ³•

    ---

    äº†è§£ å·²æœ‰çš„ `PEFT` æ–¹æ³•ã€‚

    [--> Adapters HuggingFace](https://huggingface.co/docs/peft/main/en/package_reference/adalora#peft.AdaLoraConfig)

- `PEFT` æ”¯æŒçš„ä»»åŠ¡ç±»å‹

    ---

    æ¢ç´¢ `PEFT` æ”¯æŒç”¨äºå¾®è°ƒçš„ä¸åŒä¸‹æ¸¸ä»»åŠ¡ç±»å‹ã€‚

    [--> Task_type Huggingface](https://huggingface.co/docs/peft/main/en/package_reference/peft_types#peft.TaskType)

- Get Started with PEFT

    ---

    Hugging Face å®˜æ–¹çš„çš„å‚æ•°é«˜æ•ˆå¾®è°ƒå¿«é€Ÿå…¥é—¨å’Œç¤ºä¾‹ã€‚

    [--> PEFT HuggingFace](https://huggingface.co/docs/peft/quicktour)

- `PEFT`æ–¹æ³•é»˜è®¤çš„ç›®æ ‡æ¨¡å—

    ---

    `PEFT` åº“ä¸­ `constants.py` æ–‡ä»¶å®šä¹‰äº† `PEFT` åº“ä¸­ä½¿ç”¨çš„å„ç§å¸¸é‡ï¼Œå…¶ä¸­åŒ…æ‹¬äº†é»˜è®¤çš„ç›®æ ‡æ¨¡å—ã€‚

    [--> peft.utils.constants](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py)

</div>
