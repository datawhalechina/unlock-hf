{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab_file, stop_words_file=None):\n",
    "        self.stop_words_file = self.load_stop_words(stop_words_file)\n",
    "        self.idx2word, self.word2idx, self.words = self.load_vocab(vocab_file)\n",
    "        self.vocab_size = len(self.words)\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        idx2word = {}\n",
    "        word2idx = {}\n",
    "\n",
    "        words = []\n",
    "        contents = pd.read_csv(vocab_file, encoding=\"GBK\", header=None)\n",
    "\n",
    "        for idx, row in contents.iterrows():\n",
    "            line = row[0]\n",
    "            if not self.stop_words_file:\n",
    "                current_line_words = [\n",
    "                    word for word in jieba.cut(line) if word not in self.stop_words_file\n",
    "                ]\n",
    "            else:\n",
    "                current_line_words = list(jieba.cut(line))\n",
    "            words.extend(current_line_words)\n",
    "\n",
    "        for idx, word in enumerate(set(words)):\n",
    "            idx2word[idx] = word\n",
    "            word2idx[word] = idx\n",
    "        return idx2word, word2idx, words\n",
    "\n",
    "    def load_stop_words(self, stop_words_file):\n",
    "        if stop_words_file is None:\n",
    "            return set()\n",
    "        else:\n",
    "            with open(stop_words_file, \"r\") as f:\n",
    "                return set(f.read().splitlines())\n",
    "\n",
    "    def get_idx(self, word):\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        return self.idx2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(\"./assets/数学原始数据.csv\", \"./assets/stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab: Vocab, ngram: int):\n",
    "        self.vocab = vocab\n",
    "        self.ngram = ngram\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_word = self.vocab.words[\n",
    "            self.ngram : self.vocab.vocab_size - self.ngram\n",
    "        ][idx]\n",
    "\n",
    "        idx += self.ngram\n",
    "        another_words = (\n",
    "            self.vocab.words[idx - self.ngram : idx]\n",
    "            + self.vocab.words[idx + 1 : idx + self.ngram + 1]\n",
    "        )\n",
    "        # zeros = np.zeros(self.vocab.vocab_size)\n",
    "        # zeros[self.vocab.get_idx(current_word)] = 1\n",
    "        # current_one_hot = zeros\n",
    "\n",
    "        # another_one_hot = np.zeros((self.ngram * 2, self.vocab.vocab_size))\n",
    "        # another_one_hot[\n",
    "        #     range(self.ngram * 2), [self.vocab.get_idx(x) for x in another_words]\n",
    "        # ] = 1\n",
    "\n",
    "        return {\"current_word\": current_word, \"another_words\": another_words}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MyDataset(vocab, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.model = nn.ModuleDict(\n",
    "            {\n",
    "                \"onehot2embedding\": nn.Linear(\n",
    "                    vocab_size,\n",
    "                    embedding_size,\n",
    "                    bias=True,\n",
    "                ),\n",
    "                \"embedding2logits\": nn.Linear(\n",
    "                    embedding_size,\n",
    "                    vocab_size,\n",
    "                    bias=True,\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(vocab.vocab_size, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlock-hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
