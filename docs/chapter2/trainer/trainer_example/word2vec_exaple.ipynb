{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab_file, stop_words_file=None):\n",
    "        self.stop_words_file = self.load_stop_words(stop_words_file)\n",
    "        self.idx2word, self.word2idx, self.words = self.load_vocab(vocab_file)\n",
    "        self.word_size = len(self.words)\n",
    "        self.vocab_size = len(self.idx2word)\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        idx2word = {}\n",
    "        word2idx = {}\n",
    "\n",
    "        words = []\n",
    "        contents = pd.read_csv(vocab_file, encoding=\"GBK\", header=None)\n",
    "\n",
    "        for idx, row in contents.iterrows():\n",
    "            line = row[0]\n",
    "            if not self.stop_words_file:\n",
    "                current_line_words = [\n",
    "                    word for word in jieba.cut(line) if word not in self.stop_words_file\n",
    "                ]\n",
    "            else:\n",
    "                current_line_words = list(jieba.cut(line))\n",
    "            words.extend(current_line_words)\n",
    "\n",
    "        for idx, word in enumerate(set(words)):\n",
    "            idx2word[idx] = word\n",
    "            word2idx[word] = idx\n",
    "        return idx2word, word2idx, words\n",
    "\n",
    "    def load_stop_words(self, stop_words_file):\n",
    "        if stop_words_file is None:\n",
    "            return set()\n",
    "        else:\n",
    "            with open(stop_words_file, \"r\") as f:\n",
    "                return set(f.read().splitlines())\n",
    "\n",
    "    def get_idx(self, word):\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        return self.idx2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(\"./assets/数学原始数据.csv\", \"./assets/stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.word_size, vocab.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ngram: int, vocab: Vocab):\n",
    "        self.ngram = ngram\n",
    "        self.vocab = vocab\n",
    "        self.word_size = vocab.word_size\n",
    "        self.vocab_size = vocab.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.word_size - 2 * self.ngram - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left_idx = idx\n",
    "        right_idx = idx + 2 * self.ngram + 1\n",
    "        words = self.vocab.words[left_idx:right_idx]\n",
    "        current_word = words.pop(self.ngram)\n",
    "        label = self.vocab.get_idx(current_word)\n",
    "\n",
    "        # current_word_onehot = np.zeros(self.vocab_size)\n",
    "        # current_word_onehot[self.vocab.get_idx(current_word)] = 1\n",
    "\n",
    "        another_word_onhot = np.zeros((2 * self.ngram, self.vocab_size))\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            another_word_onhot[i][self.vocab.get_idx(word)] = 1\n",
    "\n",
    "        return {\n",
    "            \"inputs\": torch.tensor(another_word_onhot, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MyDataset(2, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = torch.utils.data.DataLoader(data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.Linear(\n",
    "                embedding_size,\n",
    "                vocab_size,\n",
    "                bias=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        batch_size, ngram, vocab_size = inputs.shape\n",
    "        # [batch_size, ngram, vocab_size] -> [batch_size * ngram, vocab_size]\n",
    "        inputs = inputs.reshape(-1, self.vocab_size)\n",
    "        # [batch_size * ngram, vocab_size] -> [batch_size * ngram, vocab_size]\n",
    "        inputs_logits = self.model(inputs)\n",
    "        # [batch_size * ngram, vocab_size] -> [batch_size, ngram, vocab_size]\n",
    "        inputs_logits = inputs_logits.reshape(batch_size, ngram, vocab_size)\n",
    "        # [batch_size, ngram, vocab_size] -> [batch_size, vocab_size]\n",
    "        inputs_logits = torch.mean(inputs_logits, dim=1)\n",
    "        if labels is not None:\n",
    "            # [batch_size, vocab_size] 和 [batch_size, vocab_size]\n",
    "            loss = loss_fn(inputs_logits, labels)\n",
    "            return {\"logits\": inputs_logits, \"loss\": loss}\n",
    "        else:\n",
    "            return {\"logits\": inputs_logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(vocab.vocab_size, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallBacks(TrainerCallback):\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(\"\\nStarting training\")\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        print(\"\\nEnding training\")\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(\"\\nSaving model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./word2vec\",\n",
    "    num_train_epochs=3,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    use_cpu=False,\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data,\n",
    "    optimizers=(torch.optim.SGD(model.parameters(), 0.01), None),\n",
    "    callbacks=[MyCallBacks],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./word2vec.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlock-hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
