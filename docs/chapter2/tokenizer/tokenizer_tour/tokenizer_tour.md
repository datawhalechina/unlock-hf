---
comments: true
title: 分词
---

## 前言

在 NLP 任务中，通常处理的数据是各种各样的文本。比如下方所示的古诗，但是，模型只认数字，其他一概不认，因此我们需要找到**一种将原始文本转换为数字的方法**。这就是分词器（**tokenizer**）的职责。

```plain title='《望庐山瀑布》'
日照香炉生紫烟，遥看瀑布挂前川。
飞流直下三千尺，疑是银河落九天。
```

在分词这块，不同的语言之间存在着天然的差异，本教程主要介绍中英文分词。

基于转换方式的不同，介绍两种语言下的两种粒度的分词器。

## 中文分词

### 基于字的分词器

基于字的分词器将文本拆分为单个字。

优点：

- 字典要小得多。虽然汉字的字数基数非常庞大，但是面对<u>以字为基础，浩如烟海的词语</u>算是小巫见大巫。
- 未登记字词（OOV）要少得多。

|     |     |     | 分词  | 结果  |     |     |     |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
|  日  |  照  |  香  |  炉  |  生  |  紫  |  烟  |  ，  |
|  遥  |  看  |  瀑  |  布  |  挂  |  前  |  川  |  。  |
|  飞  |  流  |  直  |  下  |  三  |  千  |  尺  |  ，  |
|  疑  |  是  |  银  |  河  |  落  |  九  |  天  |  。  |

这种方法在实践中并不是完美的。想象一下人与人之间交流，肯定更倾向于以词为单位进行理解和表达。倘若在交流时只说一个字，对方也只回应一个字，对话双方往往对彼此所要表达的意思云里雾里。所以结合现实来说单个字的意义往往不大。

### 基于词的分词

基于词的分词器将文本拆分为多个词组。

优点：

1. 直观易懂，分词结果与人类的语言习惯相符，易于理解。
2. 处理常见词语效果好，对于词典中已有的词语，能够准确地进行分词。

缺点：

1. 处理未登录词效果差，对于词典中没有收录的新词、专业术语、网络用语等，无法进行有效分词，可能会将它们错误地切分或识别为一个词。
2. 词典构建成本高，需要人工构建和维护庞大的词典，成本较高。

|     |     | 分词结果 |     |     |
| :-: | :-: | :--: | :-: | :-: |
| 日照  | 香炉  |  生   | 紫烟  |  ，  |
| 遥看  | 瀑布  |  挂   | 前川  |  。  |
| 飞流  | 直下  | 三千尺  |     |  ，  |
| 疑是  | 银河  |  落   | 九天  |  。  |

## 英文分词

### 基于字符的分词器

基于字符的分词器将文本拆分为单个字符。

优点：

- 词汇量要小得多。英文总共 26 个字符，外加屈指可数的标点符号。
- 未登记字词（OOV）要少得多。

|     |     |     | 分词结果 |     |     |     |
| :-: | :-: | :-: | :--: | :-: | :-: | :-: |
|  L  |  e  |  t  |  '   |  s  |  d  |  o  |
|  t  |  o  |  k  |  e   |  n  |  i  |  z  |
|  a  |  t  |  i  |  o   |  n  |  !  |     |

这种方法和中文的单个字存在一样的缺陷。

### 基于词的分词

基于词的分词器将文本拆分为多个词组。其优缺点和中文基于词的分词方法相同。

|       | 分词  |      结果      |     |
| :---: | :-: | :----------: | --- |
| Let's | do  | tokenization | !   |

!!! Note
	 两种方法并非都是完美，不同的任务场景需要使用不同的分词方法。面对古诗词生成任务，开发者可能需要使用基于字的分词方法，面对日常的对话任务时，可能需要使用基于词语的分词方法。

## 子词标记化

分词算法应该依赖于这样一个原则，**即不应将常用词拆分为更小的子词，同时应将稀有词分解为有意义的子词。**

例如，“人工智能“ 可能被认为是一个罕见的词，可以分解为“人工”和“智能”。这两者都可能作为独立的子词出现得更频繁，同时“人工智能”的含义并不是“人工”和“智能”这俩子词的含义简单拼接而成的，而是由“人工”和“智能”的复合含义保持。

再比如，“annoyingly”可能被认为是一个罕见的词，可以分解为“annoying”和“ly”。这两者都可能作为独立的子词出现得更频繁，同时“annoyingly”的含义由“annoying”和“ly”的复合含义保持。

那如何让这种词语被保留下来呢？文本经过分词后，可以添加**子词标记**来标记分词后的序列。

|                  |              | 分词结果  |                   |             |
| :--------------: | :----------: | :---: | :---------------: | :---------: |
| Let's &lt;/w&gt; | do&lt;/w&gt; | token | ization&lt;/w&gt; | !&lt;/w&gt; |

子词标记化在土耳其语等粘着型语言(agglutinative languages)中特别有用，您可以通过将子词串在一起来形成（几乎）任意长的复杂词。

### 常见子词标记化方法

=== "字节对编码 (BPE)"
	BPE 从处理词汇表中的单个字符开始，然后迭代地合并最频繁的字符对，直到达到所需的词汇表大小。

	- 优点：
		1. 通过将不常见的词分解成更小的单元来有效地处理罕见词。
		2. 与基于字符的模型相比，可以生成更短的词表示，从而提高效率。

	- 缺点：
		1. 在处理具有复杂形态或大量未知词的语言时，可能会遇到困难。

=== "WordPiece"
	WordPiece 是 BERT 使用的一种标记化方法，它与 BPE 非常相似，但它基于合并后的词块在训练数据上的似然性来合并词块。

	- 优点：
		1. 与 BPE 类似，它可以很好地处理罕见词和未知词。
		2. 通常可以生成比 BPE 更紧凑的表示。

	- 缺点：
		1. 与 BPE 一样，在处理形态复杂的语言时可能会遇到困难。

=== "SentencePiece 或 Unigram"
	SentencePiece 或 Unigram（通常在多语言模型中使用）将文本视为原始字符序列，并学习将字符序列分割成单词或子词单元的概率分布。

	- 优点：
		1. 语言无关，使其适用于多语言模型。
		2. 不需要预定义的词汇表，这使其对未知词具有鲁棒性。

	- 缺点：
		1. 与 BPE 或 WordPiece 相比，生成的词表示可能更长。
		2. 可能难以捕获特定于语言的形态信息。

!!! note
	最佳标记化方法取决于具体的自然语言处理任务和所使用的语言。BPE 和 WordPiece 非常适合像英语这样的分析型语言，而 SentencePiece 则是多语言模型的更好选择。
